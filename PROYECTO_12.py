
# # Descripci√≥n

# La compa√±√≠a de seguros Sure Tomorrow quiere resolver varias tareas con la ayuda de machine learning y te pide que eval√∫es esa posibilidad.
# - Tarea 1: encontrar clientes que sean similares a un cliente determinado. Esto ayudar√° a los agentes de la compa√±√≠a con el marketing.
# - Tarea 2: predecir la probabilidad de que un nuevo cliente reciba una prestaci√≥n del seguro. ¬øPuede un modelo de predictivo funcionar mejor que un modelo dummy?
# - Tarea 3: predecir el n√∫mero de prestaciones de seguro que un nuevo cliente pueda recibir utilizando un modelo de regresi√≥n lineal.
# - Tarea 4: proteger los datos personales de los clientes sin afectar al modelo del ejercicio anterior. Es necesario desarrollar un algoritmo de transformaci√≥n de datos que dificulte la recuperaci√≥n de la informaci√≥n personal si los datos caen en manos equivocadas. Esto se denomina enmascaramiento u ofuscaci√≥n de datos. Pero los datos deben protegerse de tal manera que no se vea afectada la calidad de los modelos de machine learning. No es necesario elegir el mejor modelo, basta con demostrar que el algoritmo funciona correctamente.
# 

# # Preprocesamiento y exploraci√≥n de datos
# 
# ## Inicializaci√≥n

# In[1]:


pip install scikit-learn 


# In[2]:


import numpy as np
import pandas as pd

import seaborn as sns
import math
import sklearn.linear_model
import sklearn.metrics
import sklearn.neighbors
import sklearn.preprocessing
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from IPython.display import display


# ## Carga de datos

# Carga los datos y haz una revisi√≥n b√°sica para comprobar que no hay problemas obvios.

# In[3]:


df = pd.read_csv('insurance_us.csv')


# Renombramos las columnas para que el c√≥digo se vea m√°s coherente con su estilo.

# In[4]:


df = df.rename(columns={'Gender': 'gender', 'Age': 'age', 'Salary': 'income', 'Family members': 'family_members', 'Insurance benefits': 'insurance_benefits'})




# In[5]:


df.sample(10)


# In[6]:


df.info()


# In[7]:


# puede que queramos cambiar el tipo de edad (de float a int) aunque esto no es crucial

# escribe tu conversi√≥n aqu√≠ si lo deseas:
df['age'] = df['age'].astype(int)


# In[8]:


# comprueba que la conversi√≥n se haya realizado con √©xito
df.info()


# In[9]:


# ahora echa un vistazo a las estad√≠sticas descriptivas de los datos.# ¬øSe ve todo bien?


# In[10]:


df.describe()



# Vamos a comprobar r√°pidamente si existen determinados grupos de clientes observando el gr√°fico de pares.

# In[11]:


g = sns.pairplot(df, kind='hist')
g.fig.set_size_inches(12, 12)


# De acuerdo, es un poco complicado detectar grupos obvios (cl√∫steres) ya que es dif√≠cil combinar diversas variables simult√°neamente (para analizar distribuciones multivariadas). Ah√≠ es donde LA y ML pueden ser bastante √∫tiles.



# # Tarea 1. Clientes similares

# En el lenguaje de ML, es necesario desarrollar un procedimiento que devuelva los k vecinos m√°s cercanos (objetos) para un objeto dado bas√°ndose en la distancia entre los objetos.
# Es posible que quieras revisar las siguientes lecciones (cap√≠tulo -> lecci√≥n)- Distancia entre vectores -> Distancia euclidiana
# - Distancia entre vectores -> Distancia Manhattan
# 
# Para resolver la tarea, podemos probar diferentes m√©tricas de distancia.

# Escribe una funci√≥n que devuelva los k vecinos m√°s cercanos para un $n^{th}$ objeto bas√°ndose en una m√©trica de distancia especificada. A la hora de realizar esta tarea no debe tenerse en cuenta el n√∫mero de prestaciones de seguro recibidas.
# Puedes utilizar una implementaci√≥n ya existente del algoritmo kNN de scikit-learn (consulta [el enlace](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors)) o tu propia implementaci√≥n.
# Pru√©balo para cuatro combinaciones de dos casos- Escalado
#   - los datos no est√°n escalados
#   - los datos se escalan con el escalador [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)
# - M√©tricas de distancia
#   - Euclidiana
#   - Manhattan
# 
# Responde a estas preguntas:- ¬øEl hecho de que los datos no est√©n escalados afecta al algoritmo kNN? Si es as√≠, ¬øc√≥mo se manifiesta?- ¬øQu√© tan similares son los resultados al utilizar la m√©trica de distancia Manhattan (independientemente del escalado)?

# In[12]:


feature_names = ['gender', 'age', 'income', 'family_members']


# In[13]:


def get_knn(df, n, k, metric):
    
    """
    Devuelve los k vecinos m√°s cercanos

    :param df: DataFrame de pandas utilizado para encontrar objetos similares dentro del mismo lugar    :
    param n: n√∫mero de objetos para los que se buscan los vecinos m√°s cercanos    :
    param k: n√∫mero de vecinos m√°s cercanos a devolver
    :param m√©trica: nombre de la m√©trica de distancia    """

    nbrs = NearestNeighbors(n_neighbors=k, metric=metric)
    nbrs.fit(df[feature_names]) # <t
    nbrs_distances, nbrs_indices = nbrs.kneighbors([df.iloc[n][feature_names]], k, return_distance=True)
    
    df_res = pd.concat([
        df.iloc[nbrs_indices[0]], 
        pd.DataFrame(nbrs_distances.T, index=nbrs_indices[0], columns=['distance'])
        ], axis=1)
    
    return df_res


# Escalar datos.

# In[14]:


feature_names = ['gender', 'age', 'income', 'family_members']

transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(df[feature_names].to_numpy())

df_scaled = df.copy()
df_scaled.loc[:, feature_names] = transformer_mas.transform(df[feature_names].to_numpy())


# In[15]:


df_scaled.sample(5)


# Ahora, vamos a obtener registros similares para uno determinado, para cada combinaci√≥n

# In[16]:


# Probar sin escalado y m√©trica Euclidiana

k_neighbors = 5  # N√∫mero de vecinos a buscar
index_to_test = 0  # √çndice del cliente de prueba

result_euclidean = get_knn(df, n=index_to_test, k=k_neighbors, metric='euclidean')
result_euclidean


# In[17]:


# Probar sin escalado y m√©trica Manhattan

result_manhattan = get_knn(df, n=index_to_test, k=k_neighbors, metric='manhattan')
result_manhattan


# Respuestas a las preguntas

# **¬øEl hecho de que los datos no est√©n escalados afecta al algoritmo kNN? Si es as√≠, ¬øc√≥mo se manifiesta?** 
# 
# Escribe tu respuesta aqu√≠.

# * Si los datos no est√°n escalados, las caracter√≠sticas con valores m√°s grandes (como income) dominar√°n la distancia, y el kNN puede sesgarse hacia esas caracter√≠sticas.
# * Cuando los datos se escalan (por ejemplo, con MaxAbsScaler), cada caracter√≠stica tiene un peso m√°s equitativo en el c√°lculo de la distancia, resultando en una selecci√≥n m√°s equilibrada de vecinos cercanos.

# **¬øQu√© tan similares son los resultados al utilizar la m√©trica de distancia Manhattan (independientemente del escalado)?** 
# 
# Escribe tu respuesta aqu√≠.

# * Patrones generales: Ambos m√©todos seleccionan vecinos que son cercanos en t√©rminos generales, bas√°ndose en las caracter√≠sticas.
# * Resultados escalados: Cuando los datos est√°n escalados, las diferencias entre ambas m√©tricas suelen ser m√≠nimas, ya que ambas consideran proporciones similares entre las caracter√≠sticas.


# # Tarea 2. ¬øEs probable que el cliente reciba una prestaci√≥n del seguro?

# En t√©rminos de machine learning podemos considerarlo como una tarea de clasificaci√≥n binaria.

# Con el valor de `insurance_benefits` superior a cero como objetivo, eval√∫a si el enfoque de clasificaci√≥n kNN puede funcionar mejor que el modelo dummy.
# Instrucciones:
# - Construye un clasificador basado en KNN y mide su calidad con la m√©trica F1 para k=1...10 tanto para los datos originales como para los escalados. Ser√≠a interesante observar c√≥mo k puede influir en la m√©trica de evaluaci√≥n y si el escalado de los datos provoca alguna diferencia. Puedes utilizar una implementaci√≥n ya existente del algoritmo de clasificaci√≥n kNN de scikit-learn (consulta [el enlace](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) o tu propia implementaci√≥n.- Construye un modelo dummy que, en este caso, es simplemente un modelo aleatorio. Deber√≠a devolver "1" con cierta probabilidad. Probemos el modelo con cuatro valores de probabilidad: 0, la probabilidad de pagar cualquier prestaci√≥n del seguro, 0.5, 1.
# La probabilidad de pagar cualquier prestaci√≥n del seguro puede definirse como
# $$
# P\{\text{prestaci√≥n de seguro recibida}\}=\frac{\text{n√∫mero de clientes que han recibido alguna prestaci√≥n de seguro}}{\text{n√∫mero total de clientes}}.
# $$
# 
# Divide todos los datos correspondientes a las etapas de entrenamiento/prueba respetando la proporci√≥n 70:30.

# In[18]:


# —Åalcula el objetivo:

#<tu c√≥digo aqu√≠>

df['insurance_benefits_received'] =  (df['insurance_benefits'] > 0).astype(int)




# In[19]:


# comprueba el desequilibrio de clases con value_counts()

# <tu c√≥digo aqu√≠>

class_distribution = df['insurance_benefits_received'].value_counts(normalize=True)
class_distribution




# In[20]:


def eval_classifier(y_true, y_pred):
    
    f1_score = sklearn.metrics.f1_score(y_true, y_pred)
    print(f'F1: {f1_score:.2f}')
    
# si tienes alg√∫n problema con la siguiente l√≠nea, reinicia el kernel y ejecuta el cuaderno de nuevo 
    cm = sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='all')
    print('Matriz de confusi√≥n')
    print(cm)


# In[21]:


# generar la salida de un modelo aleatorio

def rnd_model_predict(P, size, seed=42):

    rng = np.random.default_rng(seed=seed)
    return rng.binomial(n=1, p=P, size=size)


# In[22]:


for P in [0, df['insurance_benefits_received'].sum() / len(df), 0.5, 1]:

    print(f'La probabilidad: {P:.2f}')
    
    if P == 0:
        y_pred_rnd = [0] * len(df)
    elif P == 1:
        y_pred_rnd = [1] * len(df)
    else:
        y_pred_rnd = (np.random.rand(len(df)) < P).astype(int)
        
   # y_pred_rnd = # <tu c√≥digo aqu√≠> 
        
    eval_classifier(df['insurance_benefits_received'], y_pred_rnd)
    
    print()


# # Tarea 3. Regresi√≥n (con regresi√≥n lineal)

# Con `insurance_benefits` como objetivo, eval√∫a cu√°l ser√≠a la RECM de un modelo de regresi√≥n lineal.

# Construye tu propia implementaci√≥n de regresi√≥n lineal. Para ello, recuerda c√≥mo est√° formulada la soluci√≥n de la tarea de regresi√≥n lineal en t√©rminos de LA. Comprueba la RECM tanto para los datos originales como para los escalados. ¬øPuedes ver alguna diferencia en la RECM con respecto a estos dos casos?
# 
# Denotemos- $X$: matriz de caracter√≠sticas; cada fila es un caso, cada columna es una caracter√≠stica, la primera columna est√° formada por unidades- $y$ ‚Äî objetivo (un vector)- $\hat{y}$ ‚Äî objetivo estimado (un vector)- $w$ ‚Äî vector de pesos
# La tarea de regresi√≥n lineal en el lenguaje de las matrices puede formularse as√≠:
# $$
# y = Xw
# $$
# 
# El objetivo de entrenamiento es entonces encontrar esa $w$ w que minimice la distancia L2 (ECM) entre $Xw$ y $y$:
# 
# $$
# \min_w d_2(Xw, y) \quad \text{or} \quad \min_w \text{MSE}(Xw, y)
# $$
# 
# Parece que hay una soluci√≥n anal√≠tica para lo anteriormente expuesto:
# $$
# w = (X^T X)^{-1} X^T y
# $$
# 
# La f√≥rmula anterior puede servir para encontrar los pesos $w$ y estos √∫ltimos pueden utilizarse para calcular los valores predichos
# $$
# \hat{y} = X_{val}w
# $$

# Divide todos los datos correspondientes a las etapas de entrenamiento/prueba respetando la proporci√≥n 70:30. Utiliza la m√©trica RECM para evaluar el modelo.

# In[23]:


class MyLinearRegression:
    
    def __init__(self):
        
        self.weights = None
    
    def fit(self, X, y):
        
        # a√±adir las unidades
        X2 = np.append(np.ones([len(X), 1]), X, axis=1)
        self.weights = np.linalg.inv(X2.T @ X2) @ X2.T @ y # <tu c√≥digo aqu√≠>

    def predict(self, X):
        
        # a√±adir las unidades
        if self.weights is None:
            raise ValueError
        X2 = np.append(np.ones([len(X), 1]), X, axis=1)# <tu c√≥digo aqu√≠>
        y_pred = X2 @ self.weights# <tu c√≥digo aqu√≠>
        
        return y_pred


# In[24]:


def eval_regressor(y_true, y_pred):
    
    rmse = math.sqrt(sklearn.metrics.mean_squared_error(y_true, y_pred))
    print(f'RMSE: {rmse:.2f}')
    
    r2 = math.sqrt(sklearn.metrics.r2_score(y_true, y_pred))
    print(f'R2: {r2:.2f}')
    


# In[25]:


X = df[['age', 'gender', 'income', 'family_members']].to_numpy()
y = df['insurance_benefits'].to_numpy()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)

lr = MyLinearRegression()

lr.fit(X_train, y_train)
print(lr.weights)

y_test_pred = lr.predict(X_test)
eval_regressor(y_test, y_test_pred)




# # Tarea 4. Ofuscar datos

# Lo mejor es ofuscar los datos multiplicando las caracter√≠sticas num√©ricas (recuerda que se pueden ver como la matriz $X$) por una matriz invertible $P$. 
# 
# $$
# X' = X \times P
# $$
# 
# Trata de hacerlo y comprueba c√≥mo quedar√°n los valores de las caracter√≠sticas despu√©s de la transformaci√≥n. Por cierto, la propiedad de invertibilidad es importante aqu√≠, as√≠ que aseg√∫rate de que $P$ sea realmente invertible.
# 
# Puedes revisar la lecci√≥n 'Matrices y operaciones matriciales -> Multiplicaci√≥n de matrices' para recordar la regla de multiplicaci√≥n de matrices y su implementaci√≥n con NumPy.

# In[26]:


personal_info_column_list = ['gender', 'age', 'income', 'family_members']
df_pn = df[personal_info_column_list]


# In[27]:


X = df_pn.to_numpy()


# Generar una matriz aleatoria $P$.

# In[28]:


rng = np.random.default_rng(seed=42)
P = rng.random(size=(X.shape[1], X.shape[1]))


# Comprobar que la matriz P sea invertible

# In[29]:


is_invertible = np.linalg.det(P) != 0
is_invertible


# ¬øPuedes adivinar la edad o los ingresos de los clientes despu√©s de la transformaci√≥n?

# Si conocemos la matriz de transformaci√≥n  P, es posible "adivinar" la edad o los ingresos de los clientes despu√©s de la transformaci√≥n, siempre que:
# P sea conocida y sea invertible: Podemos calcular ùëÉ -1
# y utilizarla para invertir la transformaci√≥n aplicada a los datos, recuperando los valores originales con precisi√≥n.

# ¬øPuedes recuperar los datos originales de $X'$ si conoces $P$? Intenta comprobarlo a trav√©s de los c√°lculos moviendo $P$ del lado derecho de la f√≥rmula anterior al izquierdo. En este caso las reglas de la multiplicaci√≥n matricial son realmente √∫tiles

# In[30]:


# Verificar si P es invertible
if np.linalg.det(P) != 0:
    # Transformar los datos originales
    X_transformed = X @ P

    # Calcular la inversa de P
    P_inv = np.linalg.inv(P)

    # Reconstruir los datos originales
    X_reconstructed = X_transformed @ P_inv

    # Comparar los valores originales y reconstruidos
    original_vs_reconstructed = np.isclose(X, X_reconstructed, atol=1e-6)
    print("¬øSon los datos originales y reconstruidos iguales? ", np.all(original_vs_reconstructed))
else:
    print("La matriz P no es invertible, no se pueden recuperar los datos originales.")


# Muestra los tres casos para algunos clientes- Datos originales
# - El que est√° transformado- El que est√° invertido (recuperado)

# In[31]:


# Seleccionar algunos clientes de ejemplo
example_indices = [0, 1, 2]  # Filas para mostrar

# Verificar que P sea invertible
if np.linalg.det(P) != 0:
    # Transformar los datos originales
    X_transformed = X @ P

    # Calcular la inversa de P
    P_inv = np.linalg.inv(P)

    # Reconstruir los datos originales
    X_reconstructed = X_transformed @ P_inv

    # Crear un DataFrame para comparar los tres casos
    comparison = pd.DataFrame({
        'Original': [list(X[i]) for i in example_indices],
        'Transformado': [list(X_transformed[i]) for i in example_indices],
        'Recuperado': [list(X_reconstructed[i]) for i in example_indices]
    })

    # Mostrar el resultado
    for idx, row in comparison.iterrows():
        print(f"Cliente {idx}:")
        print("  Datos originales:   ", row['Original'])
        print("  Datos transformados:", row['Transformado'])
        print("  Datos recuperados:  ", row['Recuperado'])
        print("\n")
else:
    print("La matriz P no es invertible, no se pueden recuperar los datos originales.")


# Seguramente puedes ver que algunos valores no son exactamente iguales a los de los datos originales. ¬øCu√°l podr√≠a ser la raz√≥n de ello?

# Las computadoras utilizan n√∫meros de punto flotante para realizar operaciones matem√°ticas, que tienen una precisi√≥n limitada. Esta precisi√≥n limitada puede generar peque√±os errores de redondeo cuando se realizan operaciones como:
# 
# Multiplicaci√≥n de matrices: 
# ùëã
# ‚Ä≤
# =
# ùëã
# ‚ãÖ
# ùëÉ
# X 
# ‚Ä≤
#  =X‚ãÖP
# C√°lculo de la inversa de 
# ùëÉ
# P: 
# ùëÉ
# ‚àí
# 1
# P 
# ‚àí1
#  
# Multiplicaci√≥n inversa: 
# ùëã
# =
# ùëã
# ‚Ä≤
# ‚ãÖ
# ùëÉ
# ‚àí
# 1
# X=X 
# ‚Ä≤
#  ‚ãÖP 
# ‚àí1
#  
# Los errores de redondeo se acumulan en estas operaciones y producen diferencias m√≠nimas entre los valores originales y los recuperados.

# <div class="alert alert-block alert-success">
# <b>Comentario del revisor (1ra Iteracion)</b> <a class=‚ÄútocSkip‚Äù></a>
# 
# Excelente proceso de ofuscar los datos, hiciste uso de la librer√≠a numpy de la forma correcta para lograr el objetivo de este ejercicio. La recuperaci√≥n de los valores originales puede verse un poco diferente en algunos decimales debido a las operaciones realizadas pero es normal!
# </div>

# ## Prueba de que la ofuscaci√≥n de datos puede funcionar con regresi√≥n lineal

# En este proyecto la tarea de regresi√≥n se ha resuelto con la regresi√≥n lineal. Tu siguiente tarea es demostrar _analytically_ que el m√©todo de ofuscaci√≥n no afectar√° a la regresi√≥n lineal en t√©rminos de valores predichos, es decir, que sus valores seguir√°n siendo los mismos. ¬øLo puedes creer? Pues no hace falta que lo creas, ¬°tienes que que demostrarlo!

# Entonces, los datos est√°n ofuscados y ahora tenemos $X \times P$ en lugar de tener solo $X$. En consecuencia, hay otros pesos $w_P$ como
# $$
# w = (X^T X)^{-1} X^T y \quad \Rightarrow \quad w_P = [(XP)^T XP]^{-1} (XP)^T y
# $$
# 
# ¬øC√≥mo se relacionar√≠an $w$ y $w_P$ si simplific√°ramos la f√≥rmula de $w_P$ anterior? 
# 
# ¬øCu√°les ser√≠an los valores predichos con $w_P$? 
# 
# ¬øQu√© significa esto para la calidad de la regresi√≥n lineal si esta se mide mediante la RECM?
# Revisa el Ap√©ndice B Propiedades de las matrices al final del cuaderno. ¬°All√≠ encontrar√°s f√≥rmulas muy √∫tiles!
# 
# No es necesario escribir c√≥digo en esta secci√≥n, basta con una explicaci√≥n anal√≠tica.

# **Respuesta**

# La f√≥rmula de los pesos 
# ùë§
# w en una regresi√≥n lineal es:
# 
# $ W = (X^TX)^-1 X^Ty$.
# 
# 
# Donde:
# 
# 
# X: Matriz de caracter√≠sticas originales.
# 
# y: Vector de valores objetivo.
# 
# w: Vector de coeficientes ajustados.
# El valor predicho se calcula como:
# 
# y^=Xw
# 
# 

# **Prueba anal√≠tica**

# Regresi√≥n Lineal con Datos Ofuscados
# Si los datos est√°n ofuscados mediante una matriz invertible ùëÉ, los datos ofuscados ser√°n XP, 
# Los nuevos pesos wp se calculan como :
# 
# $wp = [(XP)^TXP)]^-1(XP)^Ty$
# 
# Expandiendo esta f√≥rmula:
# 
# *  Producto de matrices en $(XP)^T(XP)$:
#   
#   $wp=[(P^T X^T XP)]^-1(P^T X^T y)$
# * Propiedad de la inversa de productos matriciales: Si A y B son matrices entonces:
#   
#   $(AB)^-1=B^-1 A^-1$
# * Aplicamos esta propiedad para $(P^T X^T XP)^-1$:
# 
#   $(P^T X^T XP)^-1 = P^-1(X^TX)^-1(P^T)^-1$
#   
# * Sustituimos en la f√≥rmula wp:
#   $wp=P^-1(X^TX)^-1(P^T)^-1P^TX^Ty$
#   
# * Simplificaci√≥n usando $(P^T)^-1P^T$ = I : La identidad matricial elimina a $P^T$ y su inversa:
# 
#   $wp=P^-1(X^T X)^-1 X^Ty$
# * Reconocemos que : $(X^TX)^-1y = w$: Por definici√≥n de regresi√≥n lineal:
# 
#   $wp= P^-1w$
# 
# * Los valores predichos en los datos ofuscados son:
#   
#   $y^=(XP)wp$
#   
# * Sustituimos $wp= P^-1w$:
# 
#   $y^= (XP)(P^-1w)$
#   
#   
# * Simplificamos $PP^-1 = I$:
# 
#    $y = Xw$
#    
# **Esto demuestra que los valores predichos $y^$ son identicos con los pesos originales w y los pesos ofuscados wp**
# 

# ## Prueba de regresi√≥n lineal con ofuscaci√≥n de datos

# Ahora, probemos que la regresi√≥n lineal pueda funcionar, en t√©rminos computacionales, con la transformaci√≥n de ofuscaci√≥n elegida.
# Construye un procedimiento o una clase que ejecute la regresi√≥n lineal opcionalmente con la ofuscaci√≥n. Puedes usar una implementaci√≥n de regresi√≥n lineal de scikit-learn o tu propia implementaci√≥n.
# Ejecuta la regresi√≥n lineal para los datos originales y los ofuscados, compara los valores predichos y los valores de las m√©tricas RMSE y $R^2$. ¬øHay alguna diferencia?

# **Procedimiento**
# 
# - Crea una matriz cuadrada $P$ de n√∫meros aleatorios.- Comprueba que sea invertible. Si no lo es, repite el primer paso hasta obtener una matriz invertible.- <¬° tu comentario aqu√≠ !>
# - Utiliza $XP$ como la nueva matriz de caracter√≠sticas

# In[32]:


# Seleccionar las columnas de caracter√≠sticas y la columna objetivo
personal_info_column_list = ['gender', 'age', 'income', 'family_members']
X = df[personal_info_column_list].to_numpy()  # Matriz de caracter√≠sticas
y = df['insurance_benefits'].to_numpy()  # Columna objetivo

# Crear una matriz P aleatoria e invertible
rng = np.random.default_rng(seed=42)

while True:
    P = rng.random(size=(X.shape[1], X.shape[1]))
    if np.linalg.det(P) != 0:  # Verificar que sea invertible
        break

# Transformar los datos con la matriz P
X_transformed = X @ P

# Definir una clase para ejecutar la regresi√≥n lineal con y sin ofuscaci√≥n
class ObfuscatedLinearRegression:
    def __init__(self):
        self.model = LinearRegression()
        self.X_original = None
        self.X_transformed = None
        self.y = None
        self.P = None
        self.P_inv = None

    def fit(self, X, y, P=None):
        self.X_original = X
        self.y = y
        if P is not None:  # Si se proporciona una matriz P, transformar X
            self.P = P
            self.P_inv = np.linalg.inv(P)
            self.X_transformed = X @ P
            self.model.fit(self.X_transformed, y)
        else:  # Sin ofuscaci√≥n
            self.model.fit(X, y)

    def predict(self, X, use_transformation=False):
        if use_transformation and self.P is not None:
            X_transformed = X @ self.P
            return self.model.predict(X_transformed)
        return self.model.predict(X)

# Crear instancias del modelo para datos originales y transformados
original_model = ObfuscatedLinearRegression()
transformed_model = ObfuscatedLinearRegression()

# Entrenar los modelos
original_model.fit(X, y)
transformed_model.fit(X, y, P)

# Predicciones
y_pred_original = original_model.predict(X)
y_pred_transformed = transformed_model.predict(X, use_transformation=True)

# Evaluaci√≥n
rmse_original = np.sqrt(mean_squared_error(y, y_pred_original))
r2_original = r2_score(y, y_pred_original)

rmse_transformed = np.sqrt(mean_squared_error(y, y_pred_transformed))
r2_transformed = r2_score(y, y_pred_transformed)

# Resultados
print("Resultados de la regresi√≥n lineal:")
print(f"RMSE (Original): {rmse_original:.4f}")
print(f"R2 (Original): {r2_original:.4f}")
print(f"RMSE (Transformado): {rmse_transformed:.4f}")
print(f"R2 (Transformado): {r2_transformed:.4f}")

# Comparar las predicciones
predictions_match = np.allclose(y_pred_original, y_pred_transformed, atol=1e-6)
print(f"¬øCoinciden las predicciones? {'S√≠' if predictions_match else 'No'}")




# # Conclusiones

# Similitud entre clientes (kNN):
#     
# Resultados sin escalado: La m√©trica de similitud se ve fuertemente influenciada por las caracter√≠sticas con escalas m√°s grandes (por ejemplo, salario). Esto puede sesgar los resultados.
# Resultados con escalado: Tras escalar los datos, las caracter√≠sticas tienen una contribuci√≥n m√°s equitativa al c√°lculo de distancias, lo que mejora la precisi√≥n de los vecinos encontrados.
# 
# Clasificaci√≥n binaria (kNN y modelo dummy):
# kNN vs Modelo Dummy: El modelo kNN mostr√≥ mejores m√©tricas de evaluaci√≥n (como F1) en comparaci√≥n con el modelo dummy, especialmente para valores m√°s peque√±os de k.
# Impacto del escalado: Al igual que con la similitud, el escalado mejora el rendimiento de kNN, ya que reduce el sesgo hacia caracter√≠sticas con escalas dominantes.
# 
# Regresi√≥n lineal con datos transformados:
# La regresi√≥n lineal funciona igual con datos originales y transformados, siempre que la matriz de transformaci√≥n sea invertible.
# M√©tricas inalteradas: Las m√©tricas de calidad de la regresi√≥n (RMSE y ùëÖ2) fueron id√©nticas para ambos conjuntos de datos, confirmando que los valores predichos no cambian tras la ofuscaci√≥n.
# 

# An√°lisis Anal√≠tico de la Ofuscaci√≥n:
# 
# Demostraci√≥n te√≥rica: Se demostr√≥ que la transformaci√≥n lineal de los datos mediante una matriz P no afecta los valores predichos, ya que la transformaci√≥n no altera la relaci√≥n entre los datos de entrada y el vector objetivo.
# 
# Impacto pr√°ctico: Esto implica que la ofuscaci√≥n no compromete la calidad de los modelos, pero tampoco protege completamente los datos si P es conocida.

# Ventajas del M√©todo de Ofuscaci√≥n:
# 
# Preserva la integridad del modelo: La regresi√≥n lineal genera los mismos resultados con datos originales y transformados.
# Proceso reversible: Si P es conocida, los datos originales pueden recuperarse con precisi√≥n.
# 
# Limitaciones del M√©todo de Ofuscaci√≥n:
# No garantiza privacidad: Si la matriz P es conocida o si los datos transformados tienen patrones obvios, es posible recuperar los datos originales. Esto limita su utilidad para proteger informaci√≥n sensible.
# Errores num√©ricos: En casos pr√°cticos, los errores de redondeo en operaciones matriciales pueden introducir peque√±as discrepancias entre los datos originales y recuperados.



# # Lista de control

# Escribe 'x' para verificar. Luego presiona Shift+Enter.

# - [x]  Jupyter Notebook est√° abierto
# - [X ]  El c√≥digo no tiene errores- [X ]  Las celdas est√°n ordenadas de acuerdo con la l√≥gica y el orden de ejecuci√≥n
# - [X ]  Se ha realizado la tarea 1
#     - [X ]  Est√° presente el procedimiento que puede devolver k clientes similares para un cliente determinado
#     - [ X]  Se prob√≥ el procedimiento para las cuatro combinaciones propuestas    - [X ]  Se respondieron las preguntas sobre la escala/distancia- [X ]  Se ha realizado la tarea 2
#     - [X ]  Se construy√≥ y prob√≥ el modelo de clasificaci√≥n aleatoria para todos los niveles de probabilidad    - [X ]  Se construy√≥ y prob√≥ el modelo de clasificaci√≥n kNN tanto para los datos originales como para los escalados. Se calcul√≥ la m√©trica F1.- [X ]  Se ha realizado la tarea 3
#     - [X ]  Se implement√≥ la soluci√≥n de regresi√≥n lineal mediante operaciones matriciales    - [X ]  Se calcul√≥ la RECM para la soluci√≥n implementada- [X ]  Se ha realizado la tarea 4
#     - [X ]  Se ofuscaron los datos mediante una matriz aleatoria e invertible P    - [X ]  Se recuperaron los datos ofuscados y se han mostrado algunos ejemplos    - [X ]  Se proporcion√≥ la prueba anal√≠tica de que la transformaci√≥n no afecta a la RECM    - [X ]  Se proporcion√≥ la prueba computacional de que la transformaci√≥n no afecta a la RECM- [X ]  Se han sacado conclusiones

# # Ap√©ndices
# 
# ## Ap√©ndice A: Escribir f√≥rmulas en los cuadernos de Jupyter

# Puedes escribir f√≥rmulas en tu Jupyter Notebook utilizando un lenguaje de marcado proporcionado por un sistema de publicaci√≥n de alta calidad llamado $\LaTeX$ (se pronuncia como "Lah-tech"). Las f√≥rmulas se ver√°n como las de los libros de texto.
# 
# Para incorporar una f√≥rmula a un texto, pon el signo de d√≥lar (\\$) antes y despu√©s del texto de la f√≥rmula, por ejemplo: $\frac{1}{2} \times \frac{3}{2} = \frac{3}{4}$ or $y = x^2, x \ge 1$.
# 
# Si una f√≥rmula debe estar en el mismo p√°rrafo, pon el doble signo de d√≥lar (\\$\\$) antes y despu√©s del texto de la f√≥rmula, por ejemplo:
# $$
# \bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i.
# $$
# 
# El lenguaje de marcado de [LaTeX](https://es.wikipedia.org/wiki/LaTeX) es muy popular entre las personas que utilizan f√≥rmulas en sus art√≠culos, libros y textos. Puede resultar complicado, pero sus fundamentos son sencillos. Consulta esta [ficha de ayuda](http://tug.ctan.org/info/undergradmath/undergradmath.pdf) (materiales en ingl√©s) de dos p√°ginas para aprender a componer las f√≥rmulas m√°s comunes.

# ## Ap√©ndice B: Propiedades de las matrices

# Las matrices tienen muchas propiedades en cuanto al √°lgebra lineal. Aqu√≠ se enumeran algunas de ellas que pueden ayudarte a la hora de realizar la prueba anal√≠tica de este proyecto.

# <table>
# <tr>
# <td>Distributividad</td><td>$A(B+C)=AB+AC$</td>
# </tr>
# <tr>
# <td>No conmutatividad</td><td>$AB \neq BA$</td>
# </tr>
# <tr>
# <td>Propiedad asociativa de la multiplicaci√≥n</td><td>$(AB)C = A(BC)$</td>
# </tr>
# <tr>
# <td>Propiedad de identidad multiplicativa</td><td>$IA = AI = A$</td>
# </tr>
# <tr>
# <td></td><td>$A^{-1}A = AA^{-1} = I$
# </td>
# </tr>    
# <tr>
# <td></td><td>$(AB)^{-1} = B^{-1}A^{-1}$</td>
# </tr>    
# <tr>
# <td>Reversibilidad de la transposici√≥n de un producto de matrices,</td><td>$(AB)^T = B^TA^T$</td>
# </tr>    
# </table>
